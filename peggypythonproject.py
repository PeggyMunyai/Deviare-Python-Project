# -*- coding: utf-8 -*-
"""PeggyPythonProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JHALyQzBUp46ygFrDSSJvoWeWqsnb4OX

# 1.Load the data file using pandas.
"""

import pandas as pd
import numpy as np
import seaborn as sns

from google.colab import files
uploaded = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(uploaded['googleplaystore.csv']))
# Dataset is now stored in a Pandas Dataframe

data = pd.read_csv('googleplaystore.csv')

data.head()

data.info()

data.shape

"""2.Check for null values in the data. Get the number of null values for each column."""

data.isnull().any()

data.isnull().sum()

"""3.Drop records with nulls in any of the columns.

3A.Dropping Null columns
"""

data = data.dropna()

"""3B.Checking null columns"""

data.isnull().any()

"""4.Variables seem to have incorrect type and inconsistent formatting. You need to fix them: 

4(1).Size column has sizes in Kb as well as Mb. To analyze, you’ll need to convert these to numeric.
"""

data["Size"] = [ float(i.split('M')[0]) if 'M' in i else float(0) for i in data["Size"]  ]

data.head()

"""4(2).Multiply the value by 1,000, if size is mentioned in Mb"""

data["Size"] = 1000 * data["Size"]

data

"""4(2).Reviews is a numeric field that is loaded as a string field. Convert it to numeric (int/float)"""

data.info()

data["Reviews"] = data["Reviews"].astype(float)

data.info()

"""4(3).Installs field is currently stored as string and has values like 1,000,000+

4(3)1 Treat 1,000,000+ as 1,000,000

"""

data["Installs"] = [ float(i.replace('+','').replace(',', '')) if '+' in i or ',' in i else float(0) for i in data["Installs"] ]

data.head()

"""4(3)2 remove ‘+’, ‘,’ from the field, convert it to integer"""

data.info()

data["Installs"] = data["Installs"].astype(int)

data.info()

"""4(4) Price field is a string and has $ symbol. Remove ‘$’ sign, and convert it to numeric."""

data['Price'] = [ float(i.split('$')[1]) if '$' in i else float(0) for i in data['Price'] ]

data.head()

data.info()

"""Convert price to int"""

data["Price"] = data["Price"].astype(int)

data.info()

"""5(1) Average rating should be between 1 and 5 as only these values are allowed on the play store. Drop the rows that have a value outside this range."""

data.shape

data.drop(data[(data['Reviews'] < 1) & (data['Reviews'] > 5 )].index, inplace = True)
data.shape

"""5(2) Reviews should not be more than installs as only those who installed can review the app. If there are any such records, drop them."""

data.shape

data.drop(data[data['Installs'] < data['Reviews'] ].index, inplace = True)
data.shape

"""5(3) For free apps (type = “Free”), the price should not be >0. Drop any such rows"""

data.shape

data.drop(data[(data['Type'] =='Free') & (data['Price'] > 0 )].index, inplace = True)
data.shape

"""5.  Performing univariate analysis:

Boxplot for Price
"""

sns.set(rc={'figure.figsize':(10,6)})
sns.boxplot(data['Price'])

"""Boxplot for Reviews"""

sns.boxplot(data['Reviews'])

"""Histogram for Rating"""

import matplotlib.pyplot as plt
import numpy as np

plt.hist(data['Rating'])
plt.show()

"""Histogram for Size"""

plt.hist(data['Size'])
plt.show()

"""6.  Outlier treatment:

6.1. Check out the records with very high price
"""

more = data.apply(lambda x : True
            if x['Price'] > 200 else False, axis = 1) 
more_count = len(more[more == True].index) 
data.shape

"""Drop Junk"""

data.drop(data[data['Price'] > 200].index, inplace = True)
data.shape

"""6.2 Reviews: Very few apps have very high number of reviews. These are all star apps that don’t help with the analysis and, in fact, will skew it. Drop records having more than 2 million reviews."""

data.drop(data[data['Reviews'] > 2000000].index, inplace = True)
data.shape

"""6.3 Installs:  There seems to be some outliers in this field too. Apps having very high number of installs should be dropped from the analysis

6.3(1)Find out the different percentiles – 10, 25, 50, 70, 90, 95, 99
"""

data.quantile([.1, .25, .5, .70, .90, .95, .99], axis = 0)

"""6(3)2 Decide a threshold as cutoff for outlier and drop records having values more than that

Drop more than 10000000
"""

data.drop(data[data['Installs'] > 10000000].index, inplace = True)
data.shape
(8496, 13)

"""7.1 Make scatter plot/joinplot for Rating vs. Price"""

sns.scatterplot(x='Rating',y='Price',data=data)

""", Paid apps have higher ratings in comparison to to free apps.

7.2 Make scatter plot/joinplot for Rating vs. Size
"""

sns.scatterplot(x='Rating',y='Size',data=data)

"""applications with bigger sizes are rated better or more

7.3 Make scatter plot/joinplot for Rating vs. Reviews
"""

sns.scatterplot(x='Rating',y='Reviews',data=data)

"""The more reviews an app has the bettter the rating.

7.4 Make boxplot for Rating vs. Content Rating
"""

sns.boxplot(x="Rating", y="Content Rating", data=data)

"""Apps which are for everyone have more bad ratings compare to others as it has so much outliers value, while 18+ apps have better ratings.

7.5 Make boxplot for Ratings vs. Category
"""

sns.boxplot(x="Rating", y="Category", data=data)

"""Events category has best ratings compare to others but Education and Art and design follow

8. Data preprocessing

8.1Reviews and Install have some values that are still relatively very high. Before building a linear regression model, you need to reduce the skew. Apply log transformation (np.log1p) to Reviews and Installs.
"""

inp1 = data
inp1.head()

inp1.skew()

reviewskew = np.log1p(inp1['Reviews'])
inp1['Reviews'] = reviewskew
reviewskew.skew()

installsskew = np.log1p(inp1['Installs'])
inp1['Installs']

installsskew.skew()

inp1.head()

"""8.3 Get dummy columns for Category, Genres, and Content Rating. This needs to be done as the models do not understand categorical data, and all data should be numeric. Dummy encoding is one way to convert character fields to numeric. Name of dataframe should be inp2."""

inp2 = inp1
inp2.head()

"""Dummy EnCoding on Column "Content Rating""""

inp2["Content Rating"].unique()

inp2['Content Rating'] = pd.Categorical(inp2['Content Rating'])

x = inp2[['Content Rating']]
del inp2['Content Rating']

dummies = pd.get_dummies(x, prefix = 'Content Rating')
inp2 = pd.concat([inp2,dummies], axis=1)
inp2.head()

"""Dummy encoding on Genre"""

inp2["Genres"].unique()

"""reduce genres"""

lists = []
for i in inp2.Genres.value_counts().index:
    if inp2.Genres.value_counts()[i]<20:
        lists.append(i)
inp2.Genres = ['Other' if i in lists else i for i in inp2.Genres] 
inp2["Genres"].unique()

inp2.Genres = pd.Categorical(inp2['Genres'])
x = inp2[["Genres"]]
del inp2['Genres']
dummies = pd.get_dummies(x, prefix = 'Genres')
inp2 = pd.concat([inp2,dummies], axis=1)
inp2.head()

"""dummy Encoding for category"""

inp2.Category.unique()

inp2.Category = pd.Categorical(inp2.Category)

x = inp2[['Category']]
del inp2['Category']

dummies = pd.get_dummies(x, prefix = 'Category')
inp2 = pd.concat([inp2,dummies], axis=1)
inp2.head()

"""9.  Train test split  and apply 70-30 split. Name the new dataframes df_train and df_test"""

from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LinearRegression as LR
from sklearn.metrics import mean_squared_error as mse

"""10. Separate the dataframes into X_train, y_train, X_test, and y_test."""

d1 = inp2
X = d1.drop('Rating',axis=1)
y = d1['Rating']

Xtrain, Xtest, ytrain, ytest = tts(X,y, test_size=0.3, random_state=5)

"""11. Model building"""

reg_all = LR()
reg_all.fit(Xtrain,ytrain)

R2_train = round(reg_all.score(Xtrain,ytrain),3)
print("The R2 value of the Training Set is : {}".format(R2_train))

"""12. Make predictions on test set and report R2."""

R2_test = round(reg_all.score(Xtest,ytest),3)
print("The R2 value of the Testing Set is : {}".format(R2_test))